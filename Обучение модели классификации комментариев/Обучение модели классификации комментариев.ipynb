{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект по теме \"Машинное обучение для текстов\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание проекта\n",
    "\n",
    "Интернет-сервис запускает функционал общения с клиентами у себя на сайте. Необходимо построить модель детектирования негативных сообщений для дальнейшей обработки сервисной службой.  \n",
    "\n",
    "### Цели \n",
    "- Обучить модель классифицировать комментарии на позитивные и негативные. В распоряжении набор данных с разметкой о токсичности сообщений.\n",
    "- Построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "1. Загрузить и подготовить данные.\n",
    "2. Обучить разные модели. \n",
    "3. Сделать выводы.\n",
    "\n",
    "\n",
    "### Описание данных\n",
    "Данные находятся в файле `toxic_comments.csv`. \n",
    "- **text** - текст комментария,\n",
    "- **toxic** - целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**План выполнения работы:**  \n",
    "- 1. Подготовка Данных  \n",
    "- 2. Обучение моделей  \n",
    "    2.1 Logistic Regression  \n",
    "    2.2 NB-SVM  \n",
    "    2.3 Linear SVC  \n",
    "- 3. Выводы  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключаем библиотеки:\n",
    "- `pandas` - для работы с таблицами  \n",
    "- `seaborn` - для визуализации данных\n",
    "- `display` - для вывода табличных данных\n",
    "- `sklearn` - инструменты машинного обучения (модели классификации, метрики для исследования качества моделей, разделение данных, предобработка данных)\n",
    "- `nltk` - для лемматизации и фильтрации стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Андрей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Андрей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Андрей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "# from deeppavlov.core.common.file import read_json\n",
    "# from deeppavlov import build_model, configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные из файла, выведем первые 10 строк таблиц для первого взгляда на данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0\n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7  Your vandalism to the Matt Shirvington article...      0\n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = pd.read_csv('toxic_comments.csv')\n",
    "comments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент объектов класса 1 к общему объёму датасета: 10.17%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Процент объектов класса 1 к общему объёму датасета: {(sum(comments['toxic']) / len(comments) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим данные для векторизации.\n",
    "- Приведём кодировку символов к Unicode\n",
    "- Проведём лемматизацию слов с помощью WordNetLemmatizer() из библиотеки `nltk`\n",
    "- Удалим пунктуацию и лишние пробелы\n",
    "- Удалим стоп-слова (пока загрузим список, удалять будем в процессе tf-idf векторизации)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = comments['text'].values.astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lemm = [lemmatize(clear_text(corpus[i])) for i in range(len(corpus))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Приводить тексты к юникоду не имеет смысла, так как они все на английском.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Разделим данные на тренировочную и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus_lemm, comments['toxic'], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочного корпуса: 127656\n",
      "Размер тренировочного корпуса: 31915\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочного корпуса: {len(X_train)}\")\n",
    "print(f\"Размер тренировочного корпуса: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Проведём векторизацию корпусов с помощью TfidfVectorizer, заодно удалим стоп-слова.\n",
    "- Попробуем обучить модель без использования n-gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(ngram_range=(1,1), stop_words=stopwords,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = tf_idf_vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = tf_idf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочного датасета: (127656, 41830)\n",
      "Размер тренировочного датасета: (31915, 41830)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочного датасета: {X_train_vec.shape}\")\n",
    "print(f\"Размер тренировочного датасета: {X_test_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём метрику accuracy для константной модели. Будем предсказывать все твиты нетоксичными ('toxic'=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy константной модели 0.898\n"
     ]
    }
   ],
   "source": [
    "base_predicts = pd.Series(data=np.zeros((len(y_test))), index=y_test.index, dtype='int16')\n",
    "base_accuacy = accuracy_score(y_test, base_predicts)\n",
    "print(f\"Accuracy константной модели {base_accuacy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Для начала попробуем обучить модель логистической регрессии. \n",
    "- Обучение, подбор гиперпараметров, кросс-валидацию проведём с помощью `GridSearchCV` библиотеки `sklearn`\n",
    "- Подбирать будем гиперпараметр регуляризации С"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   57.4s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': array([10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20.]),\n",
       "                         'max_iter': [1000]},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(10, 20, num = 11, endpoint = True),\n",
    "             'max_iter': [1000]}\n",
    "lrm = LogisticRegression()\n",
    "clf = GridSearchCV(lrm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)\n",
    "clf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучший показатель f1 на кросс-валидации : 0.773\n",
      "Параметр регуляризации для лучшей модели: {'C': 13.0, 'max_iter': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучший показатель f1 на кросс-валидации : {clf.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LogisticRegression(C=13, max_iter=1000)\n",
    "lrm.fit(X_train_vec, y_train)\n",
    "predict = lrm.predict(X_test_vec)\n",
    "f1_lr = f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.782\n"
     ]
    }
   ],
   "source": [
    "print(f\"Показатель f1 на тестовой выборке: {f1_lr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Проверим модель на адекватность. Рассчитаем метрику accuracy и сравним её с константной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy на логистической регрессии 0.960, больше, чем на константной модели\n"
     ]
    }
   ],
   "source": [
    "accuracy_lr = accuracy_score(y_test, predict)\n",
    "print(f\"Accuracy на логистической регрессии {accuracy_lr:.3f}, больше, чем на константной модели\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Показатель f1 на тестовой выборке удовлетворяет условию задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Теперь попробуем модель `NBSVM` (Naive Bayes - Support Vector Machine). В данной задаче будем испльзовать модель `LinearRegression` с алгоритмом оптимизации `solver='liblinear', dual=True`. В таком случае эти модели ведут себя похожим образом.  \n",
    "(ссылка на идею https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(x, y_i, y):\n",
    "    p = x[y==y_i].sum(axis=0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.log(prob(X_train_vec, 1, y_train.values) / prob(X_train_vec, 0, y_train.values))\n",
    "X_train_nb = X_train_vec.multiply(r)\n",
    "X_test_nb = X_test_vec.multiply(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': np.linspace(1, 11, num = 11, endpoint = True)}\n",
    "nblrm = LogisticRegression(solver='liblinear', \n",
    "                           dual=True, \n",
    "                           max_iter = 1000)\n",
    "clf_nb = GridSearchCV(nblrm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(dual=True, max_iter=1000,\n",
       "                                          solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nb.fit(X_train_nb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучший показатель f1 на кросс-валидации : 0.791\n",
      "Параметр регуляризации для лучшей модели: {'C': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучший показатель f1 на кросс-валидации : {clf_nb.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf_nb.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nblrm = LogisticRegression(C=3,\n",
    "                           solver='liblinear', \n",
    "                           dual=True,\n",
    "                           max_iter=1000)\n",
    "nblrm.fit(X_train_nb, y_train)\n",
    "predict = nblrm.predict(X_test_nb)\n",
    "f1_nblr = f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.793\n"
     ]
    }
   ],
   "source": [
    "print(f\"Показатель f1 на тестовой выборке: {f1_nblr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Данная модель мало отличается от изначальной Логистической регрессии (f1 вырос на 1%)\n",
    "- Для сравнения попробуем обучить модель `LinearSVC`. Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': np.linspace(1, 31, num = 7, endpoint = True)}\n",
    "lsvcm = LinearSVC(max_iter = 1000)\n",
    "clf_lsvc = GridSearchCV(nblrm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:   34.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=LogisticRegression(C=3, dual=True, max_iter=1000,\n",
       "                                          solver='liblinear'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'C': array([ 1.,  6., 11., 16., 21., 26., 31.])},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lsvc.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучший показатель f1 на кросс-валидации : 0.772\n",
      "Параметр регуляризации для лучшей модели: {'C': 11.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Наилучший показатель f1 на кросс-валидации : {clf_lsvc.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf_lsvc.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvcm = LogisticRegression(C=11,\n",
    "                           max_iter=1000)\n",
    "lsvcm.fit(X_train_vec, y_train)\n",
    "predict = lsvcm.predict(X_test_vec)\n",
    "f1_lsvc = f1_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.782\n"
     ]
    }
   ],
   "source": [
    "print(f\"Показатель f1 на тестовой выборке: {f1_lsvc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Молодец, что попробовал разные модели в этом шаге!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Данные о токсичности твитов успешно загружены и обработаны:\n",
    "    - Лемматизация проведена с помощью `WordNetLemmatizer` библиотеки `nltk`\n",
    "    - Знаки пунктуации, а также лишние пробелы удалены\n",
    "    - Стоп слова удалены (список взят из библиотеки `nltk`)\n",
    "    - Корпус векторизован с помощью `TfidfVectorizer`\n",
    "- На получившихся данных обучены модели: `LogisticRegression`, `NB-SVM`, `LinearSVC` \n",
    "  \n",
    "| Модель             | f1 score |  \n",
    "|:-------------------|:---------|  \n",
    "| LogisticRegression | 0.782    |  \n",
    "| NB-SVM             | 0.793    |  \n",
    "| LinearSVC          | 0.782    |  \n",
    "  \n",
    "- Качество моделей практически одинаково. Разница не более 1%. Максимальный показатель f1 получен для **NB-SVM: 0.793**\n",
    "- Кросс-валидация моделей и подбор гиперпараметров проводились с помощью GridSearchCV.\n",
    "- Проверка на адекватность была проведена для модели LogisticRegression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
